# Game Analytics: Build Your Own Warehouse

A hands-on project to build an analytics warehouse for a narrative game. You run the **game data platform** to generate and load raw data into Snowflake, then build the **dbt project** step by step: raw ‚Üí staging ‚Üí marts, macros, and tests. By the end, you‚Äôll have a working pipeline and a clear path from raw events to reporting-ready tables.

---

## The Story

![Joel and Ellie in the post-apocalyptic world of The Last of Us](images/last-of-us/img.png)

It‚Äôs **2012** in Santa Monica. You‚Äôre on the team building **The Last of Us** ‚Äî the story of Joel and Ellie is on the page, the levels are greyboxed, and combat is being tuned week by week. Design keeps asking: *Where do players die the most? Do they come back after the first session? Which chapters get abandoned?* Right now the answer is usually ‚Äúwe‚Äôll check the build and the forums.‚Äù You‚Äôve already wired the game to emit events and dump them into pipelines, but there‚Äôs no single place to go for ‚Äúhow many players, how many sessions, where they drop.‚Äù Someone has to turn that raw firehose into tables the team can actually use. That someone is you.

You‚Äôre in a lucky position: unlike the real 2012 stack, you have **modern tools**. **Snowflake** holds the raw data; **dbt** and a proper data platform are on the table. So instead of one-off scripts and spreadsheets, you get to build an **analytics warehouse** the right way ‚Äî clean, tested, and ready for the questions that will only get louder as the game nears ship.

---

**What you have to work with**

The game is at **MVP**: a few key flows are playable end-to-end, and telemetry is already flowing. You have **three core data frames** (and the pipelines that fill them):

- **Players** ‚Äî one row per player: country, language, difficulty, first seen, and other attributes. Who they are and how they chose to play.
- **Sessions** ‚Äî one row per play session. Each player has many sessions; each session has a start time, end time, platform, and duration. *When* and *how long* they play.
- **Events** ‚Äî one row per in-game event. Events belong to sessions (and thus to players): *game started*, *chapter started*, *checkpoint reached*, *enemy killed*, *player died*, *item crafted*, *chapter completed*, *game closed*. Payloads can include chapter names, locations, weather, enemy names, weapon names, crafting materials ‚Äî the stuff design will use to balance difficulty and fix friction.

The hierarchy is simple: **player ‚Üí sessions ‚Üí events**. Your job is to turn that into a warehouse: define sources, stage and clean the data, build dimensions and facts (dim_players, fct_sessions, fct_game_events), add analytics marts (DAU, funnel, retention), and harden everything with tests and CI. When you‚Äôre done, the team can stop guessing and start answering ‚Äî and you‚Äôll have done it step by step, by following the tasks below. No prior dbt experience required.

### Schema hierarchy (entities and relationships)

There are **three entities**; all relationships are **one-to-many** (no many-to-many in the core model):

| Entity   | Raw table        | Grain              | Relationship                          |
|----------|------------------|--------------------|---------------------------------------|
| **Player**  | `RAW_PLAYERS`    | One row per player | One player has **many** sessions      |
| **Session** | `RAW_SESSIONS`   | One row per session| One session has **many** events       |
| **Event**   | `RAW_GAME_EVENTS`| One row per event  | Belongs to exactly one session (and thus one player) |

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      1:N      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      1:N      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PLAYER    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ   SESSION   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ    EVENT    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§               ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§               ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ player_id   ‚îÇ               ‚îÇ session_id  ‚îÇ               ‚îÇ event_id    ‚îÇ
‚îÇ first_seen  ‚îÇ               ‚îÇ player_id   ‚îÇ               ‚îÇ session_id  ‚îÇ
‚îÇ country     ‚îÇ               ‚îÇ start/end   ‚îÇ               ‚îÇ player_id   ‚îÇ
‚îÇ language    ‚îÇ               ‚îÇ platform    ‚îÇ               ‚îÇ event_name  ‚îÇ
‚îÇ difficulty  ‚îÇ               ‚îÇ             ‚îÇ               ‚îÇ event_at    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ properties  ‚îÇ
                                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

In short: **Player 1 ‚Üí N Sessions ‚Üí N Events**. Events are tied to a session via `session_id` (and to a player via `player_id`); sessions are tied to a player via `player_id`.

### Unique values (reference data)

The synthetic pipeline uses fixed sets of values. Useful for dbt `accepted_values` tests and for understanding the data:

| Field          | Table(s)        | Unique values |
|----------------|-----------------|---------------|
| **country**    | `RAW_PLAYERS`   | `US`, `PL`, `DE`, `FR`, `ES`, `BY` |
| **language**   | `RAW_PLAYERS`   | `en`, `pl`, `de`, `fr`, `es`, `be` |
| **difficulty_selected** | `RAW_PLAYERS` | `easy`, `normal`, `hard`, `grounded` |
| **platform**   | `RAW_SESSIONS`, `RAW_GAME_EVENTS` | `ps5`, `pc` |
| **event_name** | `RAW_GAME_EVENTS` | `game_started`, `chapter_started`, `checkpoint_reached`, `enemy_killed`, `player_died`, `item_crafted`, `chapter_completed`, `game_closed` |

**properties** (in `RAW_GAME_EVENTS`) is a JSON/VARIANT; not every event has every key. Possible keys and example values:

- **chapter_name**: e.g. `The Outskirts`, `The Quarantine Zone`, `Downtown`, `The Suburbs`, `The University`, `The Hospital`, `The Financial District`, `The Docks`, `The Bridge`, `The Firefly Lab`
- **location**: e.g. `abandoned_building`, `street`, `sewer`, `rooftop`, `warehouse`, `park`, `subway`, `apartment`, `mall`, `school`
- **weather**: `clear`, `rain`, `fog`, `snow`
- **enemy_name**: e.g. `runner`, `clicker`, `bloater`, `stalker`, `shambler`, `hunter`, `soldier`, `scavenger`, `bandit`, `merchant`
- **weapon_name**: e.g. `9mm_pistol`, `revolver`, `hunting_rifle`, `assault_rifle`, `hunting_bow`, `compound_bow`
- **crafting_materials**: e.g. `alcohol`, `rag`, `scissors`, `bottle`, `tape`, `blade` (used for medkit, molotov, shiv)

---

## Quick start

If you just want to get data into Snowflake as quickly as possible, run the
bootstrap script from the root of this repo:

```bash
chmod +x run_platform.sh   # first time only
./run_platform.sh
```

This script will:

1. Prompt you for your **Snowflake credentials**.
2. Write them into `app/.env` in the expected format.
3. Create a Python virtualenv in `app/.venv` and install dependencies.
4. Run the pipeline to **generate synthetic CSVs** (players, sessions, events).
5. Run the Snowflake loader to **populate `RAW_PLAYERS`, `RAW_SESSIONS`,
   `RAW_GAME_EVENTS`**.

During the load step, the script will ask whether you want to:

- **Recreate** the RAW tables from the seed data (first run, or to reset).
- **Append** only new data on top of existing RAW tables (useful later in the
  course when you work on incremental models in Task 8).

After `run_platform.sh` finishes, your Snowflake database will have the
`RAW_*` tables ready. Next, create your dbt project and connect it to Snowflake ‚Äî see [dbt setup](instructions/dbt-setup.md).

---

## How It Works

1. **Game Data Platform** (this repo)  
   A Python pipeline that:
   - **Generates** synthetic players, sessions, and game events (CSVs).
   - **Loads** them into Snowflake as `RAW_PLAYERS`, `RAW_SESSIONS`, `RAW_GAME_EVENTS`.

   Before you run it, do the setup in this order:  
   [1) Snowflake account setup](instructions/snowflake-account-setup.md) ‚Üí [2) Pre-launch setup](instructions/pre-launch-setup.md) (database & schemas) ‚Üí [3) Snowflake credentials](instructions/snowflake-credentials.md).

2. **Your dbt project** (you create it)  
   You create a dbt project and build:
   - **Raw** ‚Äî source definitions pointing at those tables.
   - **Staging** ‚Äî views that clean, rename, and type the raw data.
   - **Marts** ‚Äî core (dimensions + facts) and analytics (DAU, funnel, retention).
   - **Macros** ‚Äî e.g. schema naming for dev/ci/prod.
   - **Tests** ‚Äî not null, unique, relationships, and a custom business rule (e.g. no overlapping sessions).

   See [dbt setup](instructions/dbt-setup.md) for how to get started with dbt.

You run the platform once to get data, then set up dbt and work through the tasks.

---

## Project Outline

| Layer / Area   | What you build |
|----------------|----------------|
| **Data in**    | Run `game-data-platform`: generate data ‚Üí load to Snowflake. |
| **Raw**        | dbt **sources**: `raw_players`, `raw_sessions`, `raw_game_events` (schema, columns, optional source tests). |
| **Staging**    | dbt **models**: `stg_players`, `stg_sessions`, `stg_game_events` ‚Äî clean, standardize names and types. |
| **Core marts** | `dim_players`, `fct_sessions`, `fct_game_events` ‚Äî dimensions and facts for analysis. |
| **Analytics marts** | `daily_active_players`, `funnel_sessions`, `retention` ‚Äî reporting-ready aggregates. |
| **Macros**     | `generate_schema_name` for environment-specific schemas. |
| **Tests**      | Column tests (not null, unique, relationships) + at least one singular test (e.g. sessions don‚Äôt overlap). |
| **CI**         | GitHub Action: `dbt compile` and optionally `dbt build` so broken contracts fail the pipeline. |

 ---
 
 ## Tasks (Do It Yourself)

Do the tasks in order. Each task is a single, clear step from start to finish.

---

### Phase 0: Get data and set up dbt

- [ ] **0.1** Clone the repo. In `app/` (this folder‚Äôs pipeline code), create a venv, install `requirements.txt`, and add a `.env` with Snowflake credentials (user, password, account, warehouse, database, schema). See [app/README.md](app/README.md) for setup.
- [ ] **0.2** From this folder (`game-data-platform/`), run `python app/main.py` (or `cd app && python main.py`) to generate CSVs and load them into Snowflake as `RAW_PLAYERS`, `RAW_SESSIONS`, `RAW_GAME_EVENTS`.
- [ ] **0.3** Create your own dbt project (see [dbt setup](instructions/dbt-setup.md)), install dbt (e.g. `pip install dbt-snowflake`), create `~/.dbt/profiles.yml` with profile `game_analytics` pointing at your Snowflake database and schema, and run `dbt deps`.

<details>
<summary>Answer ‚Äî Phase 0</summary>

*(No code ‚Äî follow the steps above. You can add notes or links here later.)*

</details>

#### Why this phase matters

- You set up the runtime pieces (virtualenv, Snowflake credentials, dbt profile) so every other step can run consistently.
- Having a dedicated `app/` and clear configs makes it easy for others to clone the repo and reproduce your warehouse.

**Progress bar (after Phase 0 ‚Äî setup done, 0/8 core phases):**

| <span style="color:#22c55e"></span><span style="color:#9ca3af">################</span> | **0% (0/8 phases complete)** |

---

### Phase 1: Raw layer (sources)

- [ ] **1.1** Create a source named `raw` that points to the three raw tables: `raw_players`, `raw_sessions`, `raw_game_events`. Set database and schema (e.g. from `target`). Add a short description per table.
- [ ] **1.2** In the same YAML, add column definitions for each source table (player_id, first_seen_at, country, language, difficulty_selected for players; session_id, player_id, session start/end, platform for sessions; event_id, event_time, player_id, event_name, platform, game_version, properties for events). Add source tests where useful: `unique` and `not_null` on primary keys, `accepted_values` on fields like difficulty or platform.

<details>
<summary>Answer ‚Äî Phase 1 (sources)</summary>

```yaml
version: 2
sources:
  - name: raw
    database: "{{ target.database }}"
    schema: "{{ target.schema }}"
    tables:
      - name: raw_players
      # raw_sessions, raw_game_events + columns & tests
```

</details>

#### Why this phase matters

- Defining sources with columns and tests turns your raw Snowflake tables into a documented contract for the rest of the project.
- Catching issues (bad values, missing keys) at the source layer keeps downstream models simpler and failures easier to debug.

**Progress bar (after Phase 1):**

| <span style="color:#22c55e">##</span><span style="color:#9ca3af">##############</span> | **12.5% (1/8 phases complete)** |

---

### Phase 2: Staging layer

- [ ] **2.1** Create **stg_players**: a view that selects from `source('raw', 'raw_players')`. Output columns: `player_id`, `first_seen_at` (cast to timestamp, e.g. `try_to_timestamp`), `country_code` (rename from `country`, uppercase), `language_code` (rename from `language`, lowercase), `difficulty_selected` (lowercase). Materialize as view.
- [ ] **2.2** Create **stg_sessions**: a view from `source('raw', 'raw_sessions')`. Output: `session_id`, `player_id`, `session_start_at`, `session_end_at` (cast to timestamp; raw columns may be named e.g. `session_start` / `session_end`), `platform` (lowercase), `session_duration_minutes` (computed as minutes between start and end). Materialize as view.
- [ ] **2.3** Create **stg_game_events**: a view from `source('raw', 'raw_game_events')`. Output: `event_id`, `event_at` (cast to timestamp; raw may be `event_time`), `player_id`, `event_name` (lowercase), `platform` (lowercase), `game_version`, `properties`. Materialize as view.
- [ ] **2.4** For each staging model, add a schema YAML with model description, column descriptions, and tests: `unique` and `not_null` on primary keys (player_id, session_id, event_id), `not_null` on important fields; for `stg_sessions`, add a `relationships` test from `player_id` to `ref('stg_players')`.

<details>
<summary>Answer ‚Äî Phase 2 (staging models)</summary>

```sql
-- stg_players.sql
{{ config(materialized='view') }}
with source as ( select * from {{ source('raw', 'raw_players') }} )
select * from source  -- add renames, try_to_timestamp, upper/lower
```

```sql
-- stg_sessions.sql
{{ config(materialized='view') }}
with source as ( select * from {{ source('raw', 'raw_sessions') }} )
select * from source  -- add session_start_at, session_end_at, session_duration_minutes
```

```sql
-- stg_game_events.sql
{{ config(materialized='view') }}
with source as ( select * from {{ source('raw', 'raw_game_events') }} )
select * from source  -- add event_at, lower(event_name), lower(platform)
```

</details>

#### Why this phase matters

- Staging models clean and standardize the raw data so all downstream tables share the same names, types, and semantics.
- This is where you remove one-off quirks from ingestion and give analytics engineers a stable, well-typed surface to build on.

**Progress bar (after Phase 2):**

| <span style="color:#22c55e">####</span><span style="color:#9ca3af">############</span> | **25% (2/8 phases complete)** | 

---

### Phase 3: Core marts

**dim_players**

- [ ] **3.1** Create model **dim_players** as a **materialized table**. Base: select all columns from `ref('stg_players')` (one row per player).
- [ ] **3.2** Add a CTE that aggregates **stg_sessions** per player: `total_sessions` (count), `total_playtime_minutes` (sum of `session_duration_minutes`), `avg_session_duration_minutes`, `first_session_at` (min of `session_start_at`), `last_session_at` (max of `session_start_at`), `active_days` (count distinct date of `session_start_at`). Group by `player_id`.
- [ ] **3.3** Join players to the session aggregates (left join). In the final select output: all player attributes, the session aggregate columns (use `coalesce(..., 0)` for counts and playtime so players with no sessions get 0), plus **days_since_first_seen** (days from `first_seen_at` to current date) and **days_since_last_session** (days from `last_session_at` to current date; null if no sessions). One row per player.

<details>
<summary>Answer ‚Äî dim_players (3.1‚Äì3.3)</summary>

```sql
{{ config(materialized='table') }}
with players as ( select * from {{ ref('stg_players') }} ),
sessions_agg as (
    select player_id, count(*) as total_sessions, sum(session_duration_minutes) as total_playtime_minutes,
           min(session_start_at) as first_session_at, max(session_start_at) as last_session_at,
           count(distinct date(session_start_at)) as active_days
    from {{ ref('stg_sessions') }} group by player_id
),
final as (
    select p.*, coalesce(s.total_sessions,0) as total_sessions, coalesce(s.total_playtime_minutes,0) as total_playtime_minutes,
           coalesce(s.active_days,0) as active_days, datediff('day', p.first_seen_at, current_timestamp()) as days_since_first_seen
    from players p left join sessions_agg s on p.player_id = s.player_id
)
select * from final
```

</details>

**fct_sessions**

- [ ] **3.4** Create model **fct_sessions** as a **materialized table**. Base: select all columns from `ref('stg_sessions')` (one row per session).
- [ ] **3.5** Join each session to **stg_players** to add `country_code`, `language_code`, `difficulty_selected` to each session row.
- [ ] **3.6** Match **stg_game_events** to sessions by `player_id` and event time within `session_start_at` and `session_end_at`. Aggregate per session: `total_events`, `unique_event_types` (count distinct event_name), `deaths_count` (count where event_name = 'player_died'), `enemies_killed` (event_name = 'enemy_killed'), `chapters_completed` (event_name = 'chapter_completed'), `first_event_at`, `last_event_at`.
- [ ] **3.7** Join sessions to the event aggregates (left join). In the final select: all session and player columns, the event aggregate columns (use `coalesce(..., 0)` for counts), and **events_per_minute** = total_events / session_duration_minutes (0 if duration is 0). One row per session.

<details>
<summary>Answer ‚Äî fct_sessions (3.4‚Äì3.7)</summary>

```sql
{{ config(materialized='table') }}
with sessions as ( select * from {{ ref('stg_sessions') }} ),
players as ( select player_id, country_code, language_code, difficulty_selected from {{ ref('stg_players') }} ),
events_agg as ( /* match events to sessions by player_id + time window, then group by session_id */ ),
final as ( select s.*, p.country_code, p.language_code, p.difficulty_selected, e.total_events, e.deaths_count, ... from sessions s left join players p ... left join events_agg e ... )
select * from final
```

</details>

**fct_game_events**

- [ ] **3.8** Create model **fct_game_events** as a **materialized table**. Base: select from `ref('stg_game_events')` (one row per event).
- [ ] **3.9** Match each event to a session: same `player_id` and `event_at` between `session_start_at` and `session_end_at`. If an event falls in multiple sessions, pick one (e.g. earliest session). Add columns: `session_id`, `session_start_at`, `session_end_at`. Events outside any session keep `session_id` (and session timestamps) null.
- [ ] **3.10** Join events to **stg_players** to add `country_code`, `language_code`, `difficulty_selected`. In the final select add **seconds_since_session_start** = seconds from `session_start_at` to `event_at` (null when session_id is null). One row per event.

<details>
<summary>Answer ‚Äî fct_game_events (3.8‚Äì3.10)</summary>

```sql
{{ config(materialized='table') }}
with events as ( select * from {{ ref('stg_game_events') }} ),
sessions as ( select session_id, player_id, session_start_at, session_end_at from {{ ref('stg_sessions') }} ),
players as ( select player_id, country_code, language_code, difficulty_selected from {{ ref('stg_players') }} ),
events_with_sessions as ( /* left join events to sessions on player_id and event_at between start/end; row_number() to pick one session per event */ ),
final as ( select *, datediff('second', session_start_at, event_at) as seconds_since_session_start from ... )
select * from final
```

</details>

- [ ] **3.11** Add schema YAML for `dim_players`, `fct_sessions`, and `fct_game_events`: column descriptions and tests. Include `unique` + `not_null` on primary keys, `relationships`: `fct_sessions.player_id` ‚Üí `dim_players.player_id`, `fct_game_events.session_id` ‚Üí `fct_sessions.session_id`.

<details>
<summary>Answer ‚Äî Core marts schema (3.11)</summary>

```yaml
# dim_players.yml / fct_sessions.yml / fct_game_events.yml
models:
  - name: dim_players
    columns:
      - name: player_id
        tests: [unique, not_null]
# + relationships on fct_sessions.player_id, fct_game_events.session_id
```

</details>

#### Why this phase matters

- Core marts (dimensions and facts) are the main interface between raw data and analytics: almost every metric is built on them.
- Getting join keys, grain, and aggregations right here prevents subtle bugs in every dashboard and analysis that follows.

**Progress bar (after Phase 3):**

| <span style="color:#22c55e">######</span><span style="color:#9ca3af">##########</span> | **37.5% (3/8 phases complete)** |

### Phase 4: Analytics marts

**daily_active_players**

- [ ] **4.1** Create **daily_active_players** as a materialized table. From **stg_sessions**, compute per (player_id, session_date, platform): session date, platform, sessions_count (count distinct session_id), total_playtime_minutes (sum duration). Join to **stg_players** for `country_code`, `difficulty_selected`.
- [ ] **4.2** In **daily_active_players**, aggregate by (session_date, platform, country_code, difficulty_selected). Output: `session_date`, `platform`, `country_code`, `difficulty_selected`, `active_players` (count distinct player_id), `total_sessions`, `total_playtime_minutes`, `avg_sessions_per_player`, `avg_playtime_minutes_per_player`. Order by session_date desc, platform, country_code, difficulty_selected.

<details>
<summary>Answer ‚Äî daily_active_players (4.1‚Äì4.2)</summary>

```sql
{{ config(materialized='table') }}
with sessions as ( select player_id, platform, date(session_start_at) as session_date, count(*) as sessions_count, sum(session_duration_minutes) as total_playtime_minutes from {{ ref('stg_sessions') }} group by 1,2,3 ),
players as ( select player_id, country_code, difficulty_selected from {{ ref('stg_players') }} ),
final as ( select s.session_date, s.platform, p.country_code, p.difficulty_selected, count(distinct s.player_id) as active_players, sum(s.sessions_count) as total_sessions, ... from sessions s left join players p ... group by ... )
select * from final order by session_date desc, platform, country_code, difficulty_selected
```

</details>

**funnel_sessions**

- [ ] **4.3** Create **funnel_sessions** as a materialized table. Match **stg_game_events** to **stg_sessions** by player_id and event time within session window. Per session compute flags: has_game_started, has_chapter_started, has_checkpoint_reached, has_chapter_completed, has_game_closed (1 if any such event, else 0), and counts: game_started_count, chapters_started_count, checkpoints_reached_count, chapters_completed_count. Join to **stg_players** for country_code, difficulty_selected.
- [ ] **4.4** In **funnel_sessions**, aggregate by (session_date, platform, country_code, difficulty_selected). Output: total_sessions, sessions_with_game_started, sessions_with_chapter_started, sessions_with_checkpoint_reached, sessions_with_chapter_completed, sessions_with_game_closed; conversion rates as percentage of total_sessions (e.g. game_started_rate_pct); avg_chapters_started, avg_checkpoints_reached, avg_chapters_completed, avg_session_duration_minutes. Order by session_date desc, platform, country_code, difficulty_selected.

<details>
<summary>Answer ‚Äî funnel_sessions (4.3‚Äì4.4)</summary>

```sql
{{ config(materialized='table') }}
with sessions as ( select ... from {{ ref('stg_sessions') }} ),
events as ( select player_id, event_name, event_at from {{ ref('stg_game_events') }} ),
session_events as ( /* join sessions + events by player_id and time window; max(case when event_name=...) as has_* */ ),
final as ( select session_date, platform, country_code, difficulty_selected, count(*) as total_sessions, sum(has_game_started), ... from session_events ... )
select * from final order by ...
```

</details>

**retention**

- [ ] **4.5** Create **retention** as a materialized table. Define cohorts from **stg_players**: cohort_date = date(first_seen_at), plus country_code, difficulty_selected. From **stg_sessions** take (player_id, session_date).
- [ ] **4.6** In **retention**: join so each (player, cohort_date, session_date) has session_date >= cohort_date. Compute days_since_cohort = datediff(day, cohort_date, session_date). Aggregate by (cohort_date, country_code, difficulty_selected, days_since_cohort): active_players (count distinct player_id), cohort_size (same for all days in that cohort). Add retention_rate_pct = (active_players / cohort_size) * 100. Order by cohort_date desc, days_since_cohort, country_code, difficulty_selected.
- [ ] **4.7** Add schema YAML for the three analytics models: descriptions and, where useful, tests (e.g. not_null on key dimensions, non-negative numeric columns).

<details>
<summary>Answer ‚Äî retention (4.5‚Äì4.6) + analytics schema (4.7)</summary>

```sql
-- retention.sql: cohorts from stg_players (cohort_date=date(first_seen_at)); join to session dates; aggregate by cohort_date, country_code, difficulty_selected, days_since_cohort; retention_rate_pct = active_players/cohort_size*100
{{ config(materialized='table') }}
with players as ( select player_id, country_code, difficulty_selected, date(first_seen_at) as cohort_date from {{ ref('stg_players') }} ),
...
```
```yaml
# analytics model .yml: not_null on session_date, platform, country_code, etc.
```

</details>

#### Why this phase matters

- Analytics marts (DAU, funnel, retention) translate raw behavioral data into the KPIs your team actually discusses.
- By keeping these as dbt models, you can iterate on definitions (e.g. what counts as active) with version control and tests instead of ad hoc SQL.

**Progress bar (after Phase 4):**

| <span style="color:#22c55e">########</span><span style="color:#9ca3af">########</span> | **50% (4/8 phases complete)** |

### Phase 5: Macros and project config

- [ ] **5.1** Add macro **generate_schema_name(custom_schema_name, node)** so that when `custom_schema_name` is set it is used, otherwise use `target.schema`. (This lets staging and marts build into different schemas when you set `+schema` in dbt_project.)
- [ ] **5.2** In `dbt_project.yml`, under `models.game_analytics`, define **staging** with `+materialized: view` and `+schema: staging`, and **marts** with `+materialized: table` and `+schema: marts`.

<details>
<summary>Answer ‚Äî Phase 5 (macro + dbt_project)</summary>

```sql
-- macros/generate_schema_name.sql
{% macro generate_schema_name(custom_schema_name, node) -%}
    {{ custom_schema_name if custom_schema_name is not none else target.schema }}
{%- endmacro %}
```
```yaml
# dbt_project.yml
models:
  game_analytics:
    staging:
      +materialized: view
      +schema: staging
    marts:
      +materialized: table
      +schema: marts
```

</details>

#### Why this phase matters

- Centralizing environment settings (schemas, materializations) keeps dev, CI, and prod aligned without copying SQL.
- Small macros like `generate_schema_name` make it easy to add more environments later without rewriting models.

**Progress bar (after Phase 5):**

| <span style="color:#22c55e">##########</span><span style="color:#9ca3af">######</span> | **62.5% (5/8 phases complete)** |

### Phase 6: Tests and quality

- [ ] **6.1** In schema YAMLs, ensure primary key columns have `unique` and `not_null`; foreign keys have `relationships` to the referenced model. Add `not_null` (or accepted_values) on other critical columns.
- [ ] **6.2** Add a **singular test** (e.g. `tests/sessions_no_overlap.sql`) that returns rows where the same player has two sessions with overlapping [session_start_at, session_end_at]. The test should fail if any such rows exist. Register it in `tests/schema.yml` and run `dbt test`.
- [ ] **6.3** Run `dbt build` locally and fix any failing models or tests until everything passes.

<details>
<summary>Answer ‚Äî Phase 6 (tests)</summary>

```sql
-- tests/sessions_no_overlap.sql: return rows where same player has two sessions with overlapping [start,end]
select * from ( ... s1 join s2 on player_id and s1.session_start < s2.session_end and s1.session_end > s2.session_start )
```
```yaml
# tests/schema.yml
tests:
  - name: sessions_no_overlap
    config: { severity: error, store_failures: true }
```

</details>

#### Why this phase matters

- Tests turn your warehouse into something you can trust: they catch broken assumptions as soon as data or code changes.
- Encoding business rules (like no overlapping sessions) in tests prevents silent data drift that would invalidate product decisions.

**Progress bar (after Phase 6):**

| <span style="color:#22c55e">############</span><span style="color:#9ca3af">####</span> | **75% (6/8 phases complete)** |

### Phase 7: CI

- [ ] **7.1** Add a GitHub Actions workflow (e.g. `.github/workflows/dbt.yml`) that on push/PR to `main`: checks out repo, sets up Python, installs dbt-snowflake, runs `dbt deps`, loads Snowflake profile from a secret (e.g. `SNOWFLAKE_CI_PROFILE`), runs `dbt compile --target ci`, and runs `dbt build --target ci`. Ensure the CI target in the profile uses a dedicated schema so broken contracts or failing tests block the merge.

<details>
<summary>Answer ‚Äî Phase 7 (CI)</summary>

```yaml
# .github/workflows/dbt.yml
name: dbt
on:
  push:    { branches: [main] }
  pull_request: { branches: [main] }
jobs:
  dbt-compile:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
      - run: pip install dbt-snowflake && dbt deps
      - run: dbt compile --target ci
      - run: dbt build --target ci
# profile from secrets.SNOWFLAKE_CI_PROFILE
```

</details>

#### Why this phase matters

- CI ensures every change to models or tests is exercised automatically before it reaches main.
- When CI fails on bad data or broken contracts, you find issues in pull requests instead of in production dashboards.

**Progress bar (after Phase 7):**

| <span style="color:#22c55e">##############</span><span style="color:#9ca3af">##</span> | **87.5% (7/8 phases complete)** |

---

### Phase 8 (Advanced): Incremental `fct_game_events`

- [ ] **8.1** Update `fct_game_events` to use `materialized='incremental'` with a stable `unique_key` (e.g. `event_id`) and a sensible `on_schema_change` strategy.
- [ ] **8.2** Implement an incremental filter with `is_incremental()` so that incremental runs only process **new** events (e.g. `event_at > max(event_at) in {{ this }}`).
- [ ] **8.3** Run a full-refresh and an incremental run, compare row counts and tests, and make sure incremental behavior matches the full build.

<details>
<summary>Answer ‚Äî Phase 8 (incremental fct_game_events)</summary>

```sql
{{ config(
  materialized = 'incremental',
  unique_key   = 'event_id',
  on_schema_change = 'ignore',  -- or 'append_new_columns' / 'sync_all_columns'
) }}

with events as (
  select * from {{ ref('stg_game_events') }}
  -- joins and enrichments here
)

{% if is_incremental() %}
  -- Only process events later than what we already have
  , filtered as (
      select *
      from events
      where event_at > (select coalesce(max(event_at), '1900-01-01') from {{ this }})
    )
  select * from filtered
{% else %}
  select * from events
{% endif %}
```

</details>

#### Why this phase matters

- Real production warehouses rarely rebuild large event tables from scratch ‚Äî **incremental models** keep compute + cost under control.
- Designing a correct incremental filter (grain, unique key, late-arriving data) forces you to think carefully about **time, idempotency, and data correctness**.

**Progress bar (after Phase 8):**

| <span style="color:#22c55e">################</span><span style="color:#9ca3af"></span> | **100% (8/8 phases complete)** |

Congratulations ‚Äî you‚Äôve completed all 8 core phases. The bar is fully green; the bonus *Final Boss* below is represented by a golden segment.

---

## üèÅ Final Boss: Product Questions You Must Be Able to Answer

![Bloater](images/last-of-us/bloater.png)

You didn't build a warehouse to admire clean models. You built it to answer hard product questions.

After completing all phases (sources ‚Üí staging ‚Üí marts ‚Üí tests ‚Üí CI), your final task is to answer the questions below **using only your dbt models**:

- `dim_players`
- `fct_sessions`
- `fct_game_events`
- `daily_active_players`
- `funnel_sessions`
- `retention`

If you can answer these ‚Äî your warehouse is working.

**Bonus progress (Final Boss questions):**

| <span style="color:#22c55e">################</span><span style="color:#eab308">####</span> | **Core 100% + Final Boss bonus** |

Green represents the 8 core phases; the golden segment corresponds to answering all of the questions in this section.

### 1. Retention & Player Return

1. What are **D1, D3, and D7 retention rates**?
2. Which **countries** have the lowest retention?
3. Does retention differ by **difficulty_selected**?
4. What % of players have **only one session**?
5. What % of players return for a **second session**?

Goal: understand whether players come back ‚Äî and who does not.

<details>
<summary>Answers ‚Äî 1. Retention & Player Return</summary>

- **Q1 ‚Äì D1/D3/D7 retention**: use the `retention` model; filter `days_since_cohort IN (1, 3, 7)` and read `retention_rate_pct` for those days (optionally averaging across cohorts).
- **Q2 ‚Äì Countries with lowest retention**: from `retention`, group by `country_code` and a chosen `days_since_cohort` (e.g. 1 or 7), order by `retention_rate_pct` ascending.
- **Q3 ‚Äì Retention by difficulty**: same `retention` model, grouped by `difficulty_selected` and `days_since_cohort`.
- **Q4 ‚Äì % of players with only one session**: from `dim_players`, compute `count(*) WHERE total_sessions = 1` divided by `count(*)` overall.
- **Q5 ‚Äì % of players returning for a second session**: from `dim_players`, compute `count(*) WHERE total_sessions >= 2` divided by `count(*)` overall.

</details>

### 2. Drop-Off & Friction

6. At which step of the session funnel do players drop the most?
7. What % of sessions never reach a **checkpoint**?
8. What % of sessions start a chapter but never complete one?
9. Are there sessions with **high deaths but low progression**?

Goal: identify friction points in the core loop.

<details>
<summary>Answers ‚Äî 2. Drop-Off & Friction</summary>

- **Q6 ‚Äì Funnel step with biggest drop**: use `funnel_sessions`; compare `sessions_with_*` columns vs `total_sessions` for each step to see where conversion rate is lowest.
- **Q7 ‚Äì % of sessions without a checkpoint**: in `funnel_sessions`, compute `1 - sessions_with_checkpoint_reached / total_sessions` for your chosen date range.
- **Q8 ‚Äì Sessions that start a chapter but never complete one**: from `funnel_sessions`, compare `sessions_with_chapter_started` vs `sessions_with_chapter_completed` and compute the difference / `total_sessions`.
- **Q9 ‚Äì High deaths, low progression sessions**: query `fct_sessions` filtering for `deaths_count` above a threshold and `chapters_completed = 0` (or very low), optionally grouping by chapter or difficulty to locate problem areas.

</details>

### 3. Difficulty & Balance

10. Which difficulty has the highest **death rate per session**?
11. Do players on higher difficulty churn faster?
12. Which chapters have the lowest completion rates?
13. Is there a relationship between **session duration** and **chapter completion**?

Goal: detect balance issues before ship.

<details>
<summary>Answers ‚Äî 3. Difficulty & Balance</summary>

- **Q10 ‚Äì Difficulty with highest death rate per session**: from `fct_sessions`, group by `difficulty_selected` and compute `sum(deaths_count) / count(*)` (or average `deaths_count`).
- **Q11 ‚Äì Do higher difficulties churn faster?**: from `retention`, group by `difficulty_selected` and `days_since_cohort` (e.g. day 1 / 7) and compare `retention_rate_pct` across difficulties.
- **Q12 ‚Äì Chapters with lowest completion**: use `fct_game_events` (or staging events) to compare counts of `chapter_started` vs `chapter_completed` by `chapter_id` / `chapter_name` and compute completion rate.
- **Q13 ‚Äì Session duration vs chapter completion**: from `fct_sessions`, bucket `session_duration_minutes` (short/medium/long) and compute `chapters_completed` or completion rate per bucket to see the relationship.

</details>

### 4. Session Behavior

14. What is the **median session duration**?
15. Are longer sessions correlated with higher retention?
16. What is the average **events_per_minute**?
17. Are there sessions with zero or unusually low event activity?

Goal: understand how players actually play.

<details>
<summary>Answers ‚Äî 4. Session Behavior</summary>

- **Q14 ‚Äì Median session duration**: read `median(session_duration_minutes)` (or approximate with `percentile_cont(0.5)`) from `fct_sessions`.
- **Q15 ‚Äì Longer sessions vs retention**: aggregate `fct_sessions` to player level (e.g. average `session_duration_minutes` per player), join to `retention` or cohort info, and compare retention metrics across duration buckets.
- **Q16 ‚Äì Average events_per_minute**: from `fct_sessions`, take `avg(events_per_minute)` (optionally by difficulty, platform, or country).
- **Q17 ‚Äì Low-activity sessions**: filter `fct_sessions` where `events_per_minute` is near zero or `total_events = 0` to find idle/buggy sessions.

</details>

### 5. Segmentation

18. Define a "core player" (e.g. ‚â•5 sessions, ‚â•120 minutes playtime, ‚â•1 chapter completed).

- How many core players exist?
- From which countries?
- On which difficulty?

19. Define "one-and-done" players (1 session, no chapter completed).

- What % of total players are they?
- Do they cluster by country or difficulty?

Goal: separate your engaged audience from early churn.

<details>
<summary>Answers ‚Äî 5. Segmentation</summary>

- **Q18 ‚Äì Core players**: in `dim_players`, define a core player as `total_sessions >= 5`, `total_playtime_minutes >= 120`, and `chapters_completed >= 1` (if you add that metric); count them and slice by `country_code` and `difficulty_selected`.
- **Q19 ‚Äì One-and-done players**: from `dim_players`, filter `total_sessions = 1` and (optionally) zero progression; compute their share of all players and break down by country and difficulty to see where early churn concentrates.

### Completion Criteria

You have successfully completed the project if:

- You can answer these questions using only warehouse models.
- You can explain *how* each answer is derived.
- Your models pass all tests.
- Your CI pipeline fails when contracts break.
- You can identify at least one actionable product insight.

At that point, you are no longer "building tables." You are running a game analytics platform.

---

## Repos at a Glance

| Path | Purpose |
|------|--------|
| **This folder** (`game-data-platform/`) | Story and task list (this README). Run `./run_platform.sh` or `python app/main.py` from here to generate and load data. Pipeline details in [app/README.md](app/README.md). |
| `app/` | Pipeline code: gen, ingest, main. See [app/README.md](app/README.md). |
| **Your dbt project** | You create it. See [dbt setup](instructions/dbt-setup.md). Then build sources ‚Üí staging ‚Üí marts; run `dbt build` after data is loaded. |

Start with the platform to get data; then set up dbt and work through the tasks in order. When you‚Äôre done, you‚Äôll have a full story: from ‚Äúraw events in Snowflake‚Äù to ‚Äútables ready for DAU, funnel, and retention.‚Äù
